{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv-tCHWy3leH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af83638f-5b2f-48e6-a973-5fbd19002e12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic Quantization"
      ],
      "metadata": {
        "id": "CFu15naKmHZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.quantization\n",
        "import torch.nn as nn\n",
        "\n",
        "# set the seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "class SampleLSTM(nn.Module):\n",
        "  \"\"\" Sample LSTM model \"\"\"\n",
        "\n",
        "  def __init__(self, in_dim, out_dim, depth):\n",
        "    super(SampleLSTM, self).__init__()\n",
        "    self.lstm = nn.LSTM(in_dim, out_dim, depth)\n",
        "\n",
        "  def forward(self, inputs, hidden):\n",
        "    out, hidden = self.lstm(inputs, hidden)\n",
        "    return out, hidden\n",
        "\n",
        "# shape parameters\n",
        "model_dimension=20\n",
        "sequence_length=10\n",
        "batch_size=1\n",
        "lstm_depth=1\n",
        "\n",
        "# random data for input\n",
        "inputs = torch.randn(sequence_length, batch_size, model_dimension)\n",
        "\n",
        "# hidden is actually is a tuple of the initial hidden state and the initial cell state\n",
        "hidden = (torch.randn(lstm_depth, batch_size, model_dimension), torch.randn(lstm_depth, batch_size,\n",
        "                                                                           model_dimension))"
      ],
      "metadata": {
        "id": "rwapEabULE-n"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here is our floating point instance\n",
        "original_lstm = SampleLSTM(model_dimension, model_dimension, lstm_depth)\n",
        "\n",
        "# apply quantization on the model\n",
        "quantized_lstm = torch.quantization.quantize_dynamic(original_lstm, {nn.LSTM, nn.Linear},\n",
        "                                                     dtype=torch.qint8)\n",
        "\n",
        "# show the changes that were made\n",
        "print('Original model:', original_lstm)\n",
        "print('Quantized model:', quantized_lstm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TICmfV9IjB_N",
        "outputId": "57e44439-ad5f-4953-b464-2d8b7f1eaef2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model: SampleLSTM(\n",
            "  (lstm): LSTM(20, 20)\n",
            ")\n",
            "Quantized model: SampleLSTM(\n",
            "  (lstm): DynamicQuantizedLSTM(20, 20)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# save the model and check the model size\n",
        "def print_size_of_model(model, label=\"\"):\n",
        "  torch.save(model.state_dict(), \"temp.p\")\n",
        "  size = os.path.getsize(\"temp.p\")\n",
        "  print(\"model: \",label, '\\t', 'Size (KB):', size/1e3)\n",
        "  os.remove('temp.p')\n",
        "  return size"
      ],
      "metadata": {
        "id": "osq8waf8kJvh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = print_size_of_model(original_lstm, \"fp32\")\n",
        "q = print_size_of_model(quantized_lstm, \"int8\")\n",
        "print(\"{0:.2f} times smaller\".format(f/q))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR17eXeRkvrM",
        "outputId": "9008dbf1-c61d-470d-c3b6-cdc1c702f62a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  fp32 \t Size (KB): 15.224\n",
            "model:  int8 \t Size (KB): 6.072\n",
            "2.51 times smaller\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare inference latency\n",
        "print(\"Floating point FP32: \", original_lstm.forward(inputs, hidden))\n",
        "print(\"Quantized INT8: \", quantized_lstm.forward(inputs, hidden))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gISmev9Sk7-i",
        "outputId": "252e88f8-8616-43d4-e2f9-250709e0e4af"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Floating point FP32:  (tensor([[[-0.0406, -0.0728,  0.0719, -0.0153, -0.0372,  0.2424,  0.1788,\n",
            "           0.0262, -0.5749,  0.2221, -0.0901, -0.2661,  0.0282,  0.3131,\n",
            "           0.0984,  0.2402, -0.4956, -0.1383,  0.3905, -0.0604]],\n",
            "\n",
            "        [[ 0.0365, -0.2455,  0.0893,  0.0454, -0.1193, -0.0401,  0.2068,\n",
            "          -0.0450, -0.2900, -0.0086, -0.0600,  0.1077, -0.0627,  0.1368,\n",
            "           0.0693,  0.1299, -0.2468, -0.1691,  0.2747,  0.0209]],\n",
            "\n",
            "        [[-0.3509, -0.1391,  0.3322, -0.0668, -0.0907,  0.0641,  0.0674,\n",
            "          -0.0018, -0.1955,  0.0989,  0.0234,  0.1796, -0.1917,  0.0570,\n",
            "          -0.0981,  0.2644, -0.0846, -0.0656,  0.1799, -0.0516]],\n",
            "\n",
            "        [[-0.0267, -0.1448,  0.2102, -0.0033,  0.0037,  0.0032,  0.1495,\n",
            "           0.2492, -0.2192, -0.0496,  0.0523,  0.0708, -0.3022,  0.0394,\n",
            "          -0.0246,  0.0592,  0.0746, -0.0228,  0.2326,  0.2190]],\n",
            "\n",
            "        [[ 0.0448, -0.0760,  0.2978,  0.0458,  0.0085, -0.0897, -0.0399,\n",
            "           0.0513, -0.2755, -0.2126,  0.0404,  0.0411, -0.2876,  0.3928,\n",
            "           0.1310,  0.0260,  0.0057, -0.2924,  0.1905,  0.0966]],\n",
            "\n",
            "        [[ 0.0120, -0.1661,  0.2634, -0.0090,  0.1200, -0.3227,  0.0597,\n",
            "           0.1070, -0.1626, -0.2411, -0.1621,  0.1899, -0.2628,  0.2658,\n",
            "           0.1051, -0.0754,  0.0009, -0.0940, -0.0020,  0.3437]],\n",
            "\n",
            "        [[ 0.1526, -0.0728,  0.2464, -0.0165,  0.1679, -0.0792,  0.0113,\n",
            "           0.1165, -0.1530, -0.0914, -0.0911, -0.0140, -0.2077,  0.2920,\n",
            "           0.1061, -0.1772,  0.0833, -0.0443,  0.1202,  0.1918]],\n",
            "\n",
            "        [[ 0.1093, -0.0499,  0.0882, -0.0849,  0.1285, -0.1838,  0.1298,\n",
            "           0.1321, -0.1015, -0.2004, -0.0467,  0.1617, -0.1926,  0.0939,\n",
            "           0.0628, -0.1684, -0.0087, -0.0447,  0.0726,  0.2748]],\n",
            "\n",
            "        [[ 0.1285,  0.1402,  0.0582, -0.0442,  0.0891, -0.2239,  0.1872,\n",
            "           0.0065, -0.0821, -0.2104, -0.0192,  0.3180, -0.1895,  0.1971,\n",
            "           0.0974, -0.1449, -0.0208, -0.0577,  0.2449,  0.2170]],\n",
            "\n",
            "        [[ 0.0909,  0.1278, -0.1082,  0.0381,  0.2978, -0.0014, -0.0140,\n",
            "           0.1346,  0.0724, -0.2695,  0.0341,  0.2075,  0.1883,  0.1963,\n",
            "           0.1286, -0.1951, -0.1800, -0.0176,  0.1140,  0.2229]]],\n",
            "       grad_fn=<MkldnnRnnLayerBackward0>), (tensor([[[ 0.0909,  0.1278, -0.1082,  0.0381,  0.2978, -0.0014, -0.0140,\n",
            "           0.1346,  0.0724, -0.2695,  0.0341,  0.2075,  0.1883,  0.1963,\n",
            "           0.1286, -0.1951, -0.1800, -0.0176,  0.1140,  0.2229]]],\n",
            "       grad_fn=<StackBackward0>), tensor([[[ 0.1989,  0.2220, -0.2478,  0.0598,  0.4590, -0.0028, -0.0395,\n",
            "           0.2007,  0.1276, -0.7162,  0.2262,  0.4561,  0.4487,  0.3662,\n",
            "           0.5651, -0.6775, -0.5295, -0.0348,  0.2173,  0.4580]]],\n",
            "       grad_fn=<StackBackward0>)))\n",
            "Quantized INT8:  (tensor([[[-4.2736e-02, -7.2882e-02,  7.4038e-02, -1.8130e-02, -3.6954e-02,\n",
            "           2.4107e-01,  1.7731e-01,  3.0890e-02, -5.7511e-01,  2.2877e-01,\n",
            "          -9.1293e-02, -2.6819e-01,  3.0348e-02,  3.1077e-01,  9.7770e-02,\n",
            "           2.4180e-01, -4.9411e-01, -1.3670e-01,  3.8759e-01, -6.1066e-02]],\n",
            "\n",
            "        [[ 3.5779e-02, -2.4633e-01,  9.1225e-02,  4.4158e-02, -1.2012e-01,\n",
            "          -4.0136e-02,  2.0543e-01, -4.5716e-02, -2.8865e-01, -5.4031e-03,\n",
            "          -5.8256e-02,  1.0782e-01, -6.2224e-02,  1.3376e-01,  6.7691e-02,\n",
            "           1.3195e-01, -2.4713e-01, -1.6502e-01,  2.7736e-01,  2.2181e-02]],\n",
            "\n",
            "        [[-3.4945e-01, -1.4094e-01,  3.3526e-01, -6.6865e-02, -9.0268e-02,\n",
            "           5.9140e-02,  6.6640e-02,  9.2148e-04, -1.9285e-01,  1.0000e-01,\n",
            "           2.3866e-02,  1.7722e-01, -1.9300e-01,  5.3229e-02, -9.8098e-02,\n",
            "           2.6430e-01, -8.1015e-02, -6.6325e-02,  1.7532e-01, -5.0575e-02]],\n",
            "\n",
            "        [[-2.7368e-02, -1.4320e-01,  2.1153e-01, -3.2644e-03,  3.7498e-03,\n",
            "          -2.1127e-03,  1.4826e-01,  2.5051e-01, -2.1659e-01, -5.0603e-02,\n",
            "           5.1491e-02,  7.2509e-02, -3.0257e-01,  4.3431e-02, -2.4460e-02,\n",
            "           5.7588e-02,  7.4727e-02, -2.3230e-02,  2.2846e-01,  2.1771e-01]],\n",
            "\n",
            "        [[ 4.5739e-02, -7.5113e-02,  3.0128e-01,  4.5442e-02,  7.7504e-03,\n",
            "          -9.2205e-02, -4.2210e-02,  5.0353e-02, -2.7358e-01, -2.1554e-01,\n",
            "           3.6108e-02,  4.1761e-02, -2.8893e-01,  3.9407e-01,  1.3028e-01,\n",
            "           2.7695e-02,  6.5264e-03, -2.9583e-01,  1.9091e-01,  9.2675e-02]],\n",
            "\n",
            "        [[ 1.3229e-02, -1.6997e-01,  2.6739e-01, -1.2543e-02,  1.1889e-01,\n",
            "          -3.2683e-01,  5.9834e-02,  1.0639e-01, -1.6307e-01, -2.4345e-01,\n",
            "          -1.6348e-01,  1.9196e-01, -2.6434e-01,  2.6284e-01,  1.0434e-01,\n",
            "          -7.4083e-02,  1.2027e-04, -9.3393e-02, -1.1243e-03,  3.4050e-01]],\n",
            "\n",
            "        [[ 1.5406e-01, -7.6062e-02,  2.4778e-01, -1.7855e-02,  1.6458e-01,\n",
            "          -7.8782e-02,  1.2635e-02,  1.1643e-01, -1.5224e-01, -9.2593e-02,\n",
            "          -9.0700e-02, -1.4171e-02, -2.0897e-01,  2.8681e-01,  1.0280e-01,\n",
            "          -1.7535e-01,  8.2062e-02, -4.2753e-02,  1.1515e-01,  1.9122e-01]],\n",
            "\n",
            "        [[ 1.1185e-01, -5.4915e-02,  8.5976e-02, -8.5604e-02,  1.2740e-01,\n",
            "          -1.8318e-01,  1.3156e-01,  1.2925e-01, -9.8196e-02, -1.9972e-01,\n",
            "          -4.7670e-02,  1.6214e-01, -1.9373e-01,  9.0014e-02,  6.2199e-02,\n",
            "          -1.6589e-01, -8.9653e-03, -4.5102e-02,  7.3169e-02,  2.7823e-01]],\n",
            "\n",
            "        [[ 1.3034e-01,  1.3568e-01,  5.5718e-02, -4.5258e-02,  8.8259e-02,\n",
            "          -2.2358e-01,  1.8848e-01,  4.3712e-03, -8.0729e-02, -2.1384e-01,\n",
            "          -2.0687e-02,  3.1686e-01, -1.8971e-01,  1.9553e-01,  9.6866e-02,\n",
            "          -1.4475e-01, -2.4752e-02, -6.3340e-02,  2.4055e-01,  2.2430e-01]],\n",
            "\n",
            "        [[ 9.1325e-02,  1.2548e-01, -1.1294e-01,  3.3623e-02,  3.0194e-01,\n",
            "          -2.2510e-03, -1.4654e-02,  1.3405e-01,  7.1294e-02, -2.7028e-01,\n",
            "           3.3310e-02,  2.0760e-01,  1.8945e-01,  1.9441e-01,  1.2808e-01,\n",
            "          -1.9734e-01, -1.8049e-01, -2.3918e-02,  1.1157e-01,  2.2019e-01]]]), (tensor([[[ 0.0913,  0.1255, -0.1129,  0.0336,  0.3019, -0.0023, -0.0147,\n",
            "           0.1340,  0.0713, -0.2703,  0.0333,  0.2076,  0.1894,  0.1944,\n",
            "           0.1281, -0.1973, -0.1805, -0.0239,  0.1116,  0.2202]]]), tensor([[[ 0.2005,  0.2175, -0.2568,  0.0526,  0.4658, -0.0046, -0.0411,\n",
            "           0.1992,  0.1264, -0.7181,  0.2233,  0.4555,  0.4517,  0.3626,\n",
            "           0.5672, -0.6831, -0.5342, -0.0475,  0.2116,  0.4526]]])))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare accuracy\n",
        "out1, hidden1 = original_lstm(inputs, hidden)\n",
        "mag1 = torch.mean(abs(out1)).item()\n",
        "print('mean absolute value of output tensor values in the FP32 model is {0:.5f} '.format(mag1))\n",
        "\n",
        "# run the quantized model\n",
        "out2, hidden2 = quantized_lstm(inputs, hidden)\n",
        "mag2 = torch.mean(abs(out2)).item()\n",
        "print('mean absolute value of output tensor values in the FP32 model is {0:.5f} '.format(mag2))\n",
        "\n",
        "# compare them\n",
        "mag3 = torch.mean(abs(out1-out2)).item()\n",
        "print(mag3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joQJYxxvlWkm",
        "outputId": "f5a3aa22-ebba-4a3c-c4ec-fa4e7e1f860b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean absolute value of output tensor values in the FP32 model is 0.13233 \n",
            "mean absolute value of output tensor values in the FP32 model is 0.13235 \n",
            "0.00181041588075459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Static Quantization"
      ],
      "metadata": {
        "id": "42AlNy6TmFqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A model with few linear layer\n",
        "class SampleLinearModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SampleLinearModel, self).__init__()\n",
        "    # QuantStub converts the incoming floating point tensors into a quantized tensor\n",
        "    self.quant = torch.quantization.QuantStub()\n",
        "    self.linear1 = torch.nn.Linear(10, 100)\n",
        "    self.linear2 = torch.nn.Linear(100, 100)\n",
        "    self.linear3 = torch.nn.Linear(100, 100)\n",
        "    self.linear4 = torch.nn.Linear(100, 100)\n",
        "    self.linear5 = torch.nn.Linear(100, 1)\n",
        "\n",
        "    # DeQuanStub converts the given quantized tensor into a tensor in floating point\n",
        "    self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # using QuantStub and DeQuanStub operations, we can indicate the region for quantization\n",
        "    # point to quantized in the quantized model\n",
        "    x = self.quant(x)\n",
        "    x = self.linear1(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.linear3(x)\n",
        "    x = self.linear4(x)\n",
        "    x = self.linear5(x)\n",
        "    x = self.dequant(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "zbHxHxVll8Eb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare model for static quantization\n",
        "original_model = SampleLinearModel()\n",
        "print(original_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSol5X5ymR4a",
        "outputId": "8faa698d-5cf9-4576-b8dd-a2231c035165"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SampleLinearModel(\n",
            "  (quant): QuantStub()\n",
            "  (linear1): Linear(in_features=10, out_features=100, bias=True)\n",
            "  (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (linear3): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (linear4): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (linear5): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (dequant): DeQuantStub()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply Quantization"
      ],
      "metadata": {
        "id": "k-TTs4Nmnn6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCalibrationDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self):\n",
        "    self.num_samples=100\n",
        "    self.data=torch.rand([self.num_samples, 10])\n",
        "    self.label=torch.rand([self.num_samples, 1])\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.num_samples\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx], self.label[idx]\n",
        "\n",
        "calibration_dataset = CustomCalibrationDataset()\n",
        "calibration_data_loader = torch.utils.data.DataLoader(calibration_dataset)"
      ],
      "metadata": {
        "id": "FpuMc0WEniWr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model.eval()\n",
        "original_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "quantized_model = torch.quantization.prepare(original_model)\n",
        "\n",
        "quantized_model.eval()\n",
        "for data, label in calibration_data_loader:\n",
        "  quantized_model(data)\n",
        "\n",
        "torch.quantization.convert(quantized_model, inplace=True)\n",
        "print(quantized_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI3jyTDuoijx",
        "outputId": "d95b7376-8fd6-4dd7-d456-e179233d18fc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SampleLinearModel(\n",
            "  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
            "  (linear1): QuantizedLinear(in_features=10, out_features=100, scale=0.020363805815577507, zero_point=60, qscheme=torch.per_channel_affine)\n",
            "  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.012322427704930305, zero_point=65, qscheme=torch.per_channel_affine)\n",
            "  (linear3): QuantizedLinear(in_features=100, out_features=100, scale=0.006431824527680874, zero_point=62, qscheme=torch.per_channel_affine)\n",
            "  (linear4): QuantizedLinear(in_features=100, out_features=100, scale=0.004961901344358921, zero_point=58, qscheme=torch.per_channel_affine)\n",
            "  (linear5): QuantizedLinear(in_features=100, out_features=1, scale=0.0001570889144204557, zero_point=127, qscheme=torch.per_channel_affine)\n",
            "  (dequant): DeQuantize()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare model size\n",
        "f = print_size_of_model(original_model, \"fp32\")\n",
        "q = print_size_of_model(quantized_model, \"int8\")\n",
        "print(\"{0:.2f} times smaller\".format(f/q))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxEkqRAdo6j9",
        "outputId": "7abdfa8a-a9d8-459a-f44b-b8cdcbd98122"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  fp32 \t Size (KB): 15.224\n",
            "model:  int8 \t Size (KB): 6.072\n",
            "2.51 times smaller\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization aware training in Pytorch"
      ],
      "metadata": {
        "id": "7oM8IlsypVsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a sample model\n",
        "original_model = SampleLinearModel()\n",
        "\n",
        "training_dataset = CustomCalibrationDataset()\n",
        "training_data_loader = torch.utils.data.DataLoader(calibration_dataset, 5)"
      ],
      "metadata": {
        "id": "y4ataaCDpHwH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply quantization"
      ],
      "metadata": {
        "id": "VQHxZ3nIpo-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_model.train()\n",
        "original_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "quantized_model = torch.quantization.prepare_qat(original_model)\n",
        "\n",
        "# train the model\n",
        "quantized_model.train()\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(original_model.parameters(), lr=0.001, momentum=0.9)\n",
        "for data, label in training_data_loader:\n",
        "  optimizer.zero_grad()\n",
        "  pred = quantized_model(data)\n",
        "  loss = mse_loss(pred, label)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "quantized_model.eval()\n",
        "torch.quantization.convert(quantized_model, inplace=True)\n",
        "print(quantized_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjZI3eOepmzO",
        "outputId": "6f3b8575-233f-48d8-effc-9d77d7fa7bbf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SampleLinearModel(\n",
            "  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
            "  (linear1): QuantizedLinear(in_features=10, out_features=100, scale=0.02046726830303669, zero_point=62, qscheme=torch.per_channel_affine)\n",
            "  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.011270151473581791, zero_point=68, qscheme=torch.per_channel_affine)\n",
            "  (linear3): QuantizedLinear(in_features=100, out_features=100, scale=0.007421422284096479, zero_point=65, qscheme=torch.per_channel_affine)\n",
            "  (linear4): QuantizedLinear(in_features=100, out_features=100, scale=0.004118180833756924, zero_point=56, qscheme=torch.per_channel_affine)\n",
            "  (linear5): QuantizedLinear(in_features=100, out_features=1, scale=0.0009085990022867918, zero_point=0, qscheme=torch.per_channel_affine)\n",
            "  (dequant): DeQuantize()\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare model size"
      ],
      "metadata": {
        "id": "PbhQ4V7Cqmaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare model size\n",
        "f = print_size_of_model(original_model, \"fp32\")\n",
        "q = print_size_of_model(quantized_model, \"int8\")\n",
        "print(\"{0:.2f} times smaller\".format(f/q))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfsbC3LGqepT",
        "outputId": "e5ff099b-f49c-4c64-a5c9-56391544fdf2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  fp32 \t Size (KB): 129.422\n",
            "model:  int8 \t Size (KB): 48.586\n",
            "2.66 times smaller\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LV2fW4Vuqpoz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}