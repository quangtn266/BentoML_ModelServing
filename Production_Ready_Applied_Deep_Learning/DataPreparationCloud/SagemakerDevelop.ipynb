{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "xv-tCHWy3leH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350af0e8-32f8-46ce-8f3e-f3639d15be00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZB6UdDBIdjF",
        "outputId": "781994ab-f784-44a6-cfa7-24c581be06c6"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, os, sys\n",
        "import sagemaker, boto3\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yHy7gYpI_zO",
        "outputId": "ca621ca5-c814-4e7b-8792-510e6a4463ff"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
            "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['AWS_DEFAULT_REGION']='us-east-2'"
      ],
      "metadata": {
        "id": "sONo7a9tKi8z"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sess = boto3.Session()\n",
        "sm   = sess.client('sagemaker', region_name='us-east-2')\n",
        "# get execution role\n",
        "#role = sagemaker.get_execution_role()\n",
        "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
        "\n",
        "datasets = sagemaker_session.upload_data(path='cifar10', key_prefix='dataset/cifar10-dataset')\n",
        "datasets"
      ],
      "metadata": {
        "id": "o8__yLVgJZEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from smexperiments.experiment import Experiment\n",
        "from smexperiments.trial import Trial\n",
        "from smexperiments.trial_component import TrialComponent"
      ],
      "metadata": {
        "id": "Myp0-lG9KIx9"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change the experiment_name if needed. Experiment name has to be unique within and AWS account and AWS region\n",
        "training_experiment = Experiment.create(experiment_name=\"sagemaker-experiments-v1\", description=\"Experiment to track cifar10 training trials\",\n",
        "                                        sagemaker_boto_client=sm)\n",
        "\n",
        "#### setup trials\n",
        "l_experiment_name = traininig_experiment.experiment_name\n",
        "print(l_experiment_name)\n",
        "\n",
        "# trial name should be unique\n",
        "single_gpu_trial = Trial.create(trial_name=\"sagemake-single-gpu-training-v1\", experiment_name=training_experiment.experiment_name,\n",
        "                                sagemaker_boto_client=sm,)\n",
        "\n",
        "trial_comp_name = \"single-gpu-training-job\"\n",
        "experiment_config = {\"ExperiementName\": training_epxeriment.experiment_name,\n",
        "                     \"TrialName\": single_gpu_trial.trial_name,\n",
        "                     \"TrialComponentDisplayName\": trial_comp_name}"
      ],
      "metadata": {
        "id": "IfPfc4kELg94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import time"
      ],
      "metadata": {
        "id": "qM3sy2FCLkXa"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HEIGHT=32\n",
        "WIDTH=32\n",
        "DEPTH=3\n",
        "NUM_CLASSES=10"
      ],
      "metadata": {
        "id": "v70nYRTeNT_0"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def single_example_parser(serialized_example):\n",
        "  \"\"\" Parse a single tf.example into image abd label tensors. \"\"\"\n",
        "  \"\"\" Dimensions of the images in the CIFAR dataset. \"\"\"\n",
        "  features = tf.io.parse_single_example(\n",
        "      serialized_example,\n",
        "      features = {\n",
        "          'image': tf.io.FixedLenFeature([], tf.string),\n",
        "          'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "      }\n",
        "  )\n",
        "\n",
        "  image = tf.io.decode_raw(features['image'], tf.uint8)\n",
        "  image.set_shape([DEPTH*HEIGHT*WIDTH])\n",
        "\n",
        "  # reshape from [depth * height * width] to [depth, height, width].\n",
        "  image = tf.cast(tf.transpose(tf.reshape(image, [DEPTH, HEIGHT, WIDTH]), [1, 2, 0]), tf.float32)\n",
        "  label = tf.cast(features['label'], tf.int32)\n",
        "\n",
        "  image =train_preprocess_fn(image)\n",
        "  label = tf.one_hot(label, NUM_CLASSES)\n",
        "\n",
        "  return image, label"
      ],
      "metadata": {
        "id": "g1fuRgF4NbPO"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_preprocess_fn(image):\n",
        "  # resize the image to add 4 extra pixels on each side.\n",
        "  image = tf.image.resize_with_crop_or_pad(image, HEIGHT+8, WIDTH+8)\n",
        "\n",
        "  # Randomly crop [HEIGHT, WIDTH] section of the image\n",
        "  image = tf.image.random_crop(image, [HEIGHT, WIDTH, DEPTH])\n",
        "\n",
        "  # Randomly flip the image horizontally.\n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "  return image"
      ],
      "metadata": {
        "id": "MIoY_qKOPP1e"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(filename, batch_size):\n",
        "  \"\"\" Read the images and labels from 'filenames' \"\"\"\n",
        "  # repeat infinitely.\n",
        "  dataset = tf.data.TFRecordDataset(filename).repeat().shuffle(10000)\n",
        "\n",
        "  #parse records\n",
        "  dataset = dataset.map(single_example_parser, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # Batch it up\n",
        "  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "OgiaQWIkP249"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(input_shape, learning_rate, weight_decay, optimizer, momentum):\n",
        "    input_tensor = Input(shape=input_shape)\n",
        "    base_model = keras.applications.resnet50.ResNet50(include_top=False,\n",
        "                                                          weights='imagenet',\n",
        "                                                          input_tensor=input_tensor,\n",
        "                                                          input_shape=input_shape,\n",
        "                                                          classes=None)\n",
        "    x = Flatten()(base_model.output)\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model"
      ],
      "metadata": {
        "id": "ALJ4HYSAQ_C0"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  # Hyper parameters\n",
        "  epochs = 10\n",
        "  lr = 0.01\n",
        "  batch_size = 16\n",
        "  momentum = 2e-4\n",
        "  weight_decay = 0.9\n",
        "  optimizer = 'sgd'\n",
        "  model_dir = 's3://sagemaker-us-east-2-058199717680/models/cifar10/'\n",
        "\n",
        "  # Sagemaker options\n",
        "  training_dir   = 's3://sagemaker-us-east-2-058199717680/datasets/cifar10-dataset'\n",
        "  validation_dir = 's3://sagemaker-us-east-2-058199717680/datasets/cifar10-dataset'\n",
        "  eval_dir       = 's3://sagemaker-us-east-2-058199717680/datasets/cifar10-dataset'\n",
        "\n",
        "  print(f\"training: {training_dir} valid: {validation_dir} | eval: {eval_dir}\")\n",
        "  print(f\"optimizer: {optimizer}\")\n",
        "\n",
        "  train_dataset = get_dataset(training_dir+'/train.tfrecords',  batch_size)\n",
        "  val_dataset   = get_dataset(validation_dir+'/validation.tfrecords', batch_size)\n",
        "  eval_dataset  = get_dataset(eval_dir+'/eval.tfrecords', batch_size)\n",
        "\n",
        "  input_shape = (HEIGHT, WIDTH, DEPTH)\n",
        "  model = get_model(input_shape, lr, weight_decay, optimizer, momentum)\n",
        "\n",
        "  # optimizer\n",
        "  if optimizer.lower() == 'sgd':\n",
        "    opt = tf.keras.optimizers.legacy.SGD(learning_rate=lr, decay=weight_decay, momentum=momentum)\n",
        "  else:\n",
        "    opt = tf.keras.optimizers.legacy.Adam(learning_rate=lr, decay=weight_decay)\n",
        "\n",
        "  # compile the model\n",
        "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # train model\n",
        "  history = model.fit(train_dataset, steps_per_epoch=40000 // batch_size,\n",
        "                        validation_data=val_dataset,\n",
        "                        validation_steps=10000 // batch_size,\n",
        "                        epochs=epochs)\n",
        "\n",
        "  # evaluate model performance\n",
        "  score = model.evaluate(eval_dataset, steps=1000//batch_size, verbose=1)\n",
        "  print(\"test loss: \", score[0])\n",
        "  print(\"test accuracy: \", score[1])\n",
        "\n",
        "  # save the model to model directory\n",
        "  model.save(f'{model_dir}/{time.strftime(\"%m%d%H%M%S\", time.gmtime())}', save_format='tf')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "DBCtGghuRo_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.tensorflow import TensorFlow"
      ],
      "metadata": {
        "id": "D-vu-4GXUHne"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparams={'epochs': 30,\n",
        "            'learning-rate': 0.01,\n",
        "             'batch-size': 16,\n",
        "             'weight-decay': 2e-4,\n",
        "             'momentum': 0.9,\n",
        "             'optmizer': 'adam'\n",
        "}\n",
        "\n",
        "bucket_name = sagemaker_session.default_bucket()\n",
        "output_path = f's3://{bucket_name}/jobs'\n",
        "metrics_definitions = [{'Name': 'val_acc', 'Regex': 'val_acc: ([0-9\\\\.]+)'}]\n",
        "\n",
        "tf_estimator = TensorFlow(entry_point = 'cifar10-training-sagemaker.py',\n",
        "                          output_path = f'{output_path}/',\n",
        "                          code_location = output_path,\n",
        "                          role = role,\n",
        "                          train_instance_count = 1,\n",
        "                          train_instance_type = 'ml.g4dn.xlarge',\n",
        "                          pv_version = 'py3',\n",
        "                          script_mode = True,\n",
        "                          metric_definitions = metrics_definitions,\n",
        "                          sagemaker_ession = sagemaker_session,\n",
        "                          hyperparameters = hyperparams\n",
        "                          )\n",
        "\n",
        "\n",
        "job_name = f'tensorflow-single-gpu-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())}'\n",
        "tf_estimator({'training': datasets,\n",
        "              'validation': datasets,\n",
        "              'eval': datasets},\n",
        "             job_name = job_name,\n",
        "             experiment_config = experiment_config\n",
        "             )"
      ],
      "metadata": {
        "id": "1WOzLc7dWgPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2qTDO3WLZFd7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}